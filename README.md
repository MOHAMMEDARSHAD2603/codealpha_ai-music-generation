# ğŸ¶ AI Music Generation with LSTM | CodeAlpha Internship

This project trains a deep learning model to generate music using classical MIDI files. It was built as part of the CodeAlpha internship and demonstrates the full pipeline from data collection to music playback.

---

## ğŸ“ Project Structure
- `music_gen.py` â€” trains the LSTM model and generates new music.
- `midi_songs/` â€” folder containing training MIDI files (classical pieces).
- `generated_music.mid` â€” output MIDI file from the model.
- `play_midi.py` â€” script to auto-play the generated MIDI.
- `inspect_midi.py` â€” prints note sequences for debugging.
- `midi_to_wav.py` â€” optional script to convert MIDI to WAV using FluidSynth.
- `app.py` â€” Streamlit app for interactive demo.
- `requirements.txt` â€” dependencies for running the project.

---

## ğŸ§  Model Overview
- Framework: PyTorch  
- Architecture: LSTM (Recurrent Neural Network)  
- Input: Preprocessed note sequences from MIDI files  
- Output: New note sequences saved as MIDI  

---

## ğŸš€ Getting Started

**Prerequisites**
- Python 3.12
- pip (Python package manager)

**Installation**
```bash
git clone https://github.com/MOHAMMEDARSHAD2603/codealpha_ai-music-generation.git
cd codealpha_ai-music-generation
pip install -r requirements.txt

Run Training + Generation
python music_gen.py


Play Output
python play_midi.py


Optional: Convert to WAV
python midi_to_wav.py


ğŸ§ Usage
- Collect MIDI files in midi_songs/
- Train the LSTM model with music_gen.py
- Generate new music â†’ saved as generated_music.mid
- Play or convert to WAV for polished audio
- Try the interactive demo via app.py (Streamlit)

ğŸ”® Future Improvements
- Add GAN-based music generation for richer compositions
- Build a Streamlit web app with real-time playback
- Expand dataset with multiple genres (jazz, pop, Indian classical)
- Add download button for generated audio in the app
- Deploy as a mobile app with PyTorch Mobile


ğŸ“Œ Author
Mohammed Arshad.R
Coimbatore, Tamil Nadu, India
CodeAlpha Internship Project, 2025



